{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98360655737704924Epoch 0 : Time 10.990955114364624 : loss 0.97286774858832359 ; valid_loss 4.6291896139874176\n",
      "Epoch 1 : Time 10.754860401153564 : loss 1.247023033797741 ; valid_loss 2.1545129244161\n",
      "Epoch 2 : Time 10.774378061294556 : loss 1.1005824652314187 ; valid_loss 0.69764198581962023\n",
      "Epoch 3 : Time 10.756994724273682 : loss 0.62783252716064453 ; valid_loss 1.1249279237407095\n",
      "Epoch 4 : Time 10.724502325057983 : loss 0.484947469830513 ; valid_loss 1.7557484380462591\n",
      "Epoch 5 : Time 10.737718343734741 : loss 0.45159276679158211 ; valid_loss 1.3277852312278222\n",
      "Epoch 6 : Time 10.764772891998291 : loss 0.390278914347291 ; valid_loss 0.893753783284303\n",
      "Epoch 7 : Time 10.617450952529907 : loss 0.34778528116643431 ; valid_loss 0.64264728764400758\n",
      "Epoch 8 : Time 10.65851616859436 : loss 0.2894246181845665 ; valid_loss 0.59958607155610533\n",
      "Epoch 9 : Time 10.746488332748413 : loss 0.255523500777781 ; valid_loss 0.67511584281044845\n",
      "Epoch 10 : Time 10.683893918991089 : loss 0.2516524036601186 ; valid_loss 0.76453943517716494\n",
      "Epoch 11 : Time 10.715821504592896 : loss 0.23855199672281743 ; valid_loss 0.69683879438568563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-048e37cbc1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m \u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-048e37cbc1b6>\u001b[0m in \u001b[0;36mtrainingz\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss_metric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-048e37cbc1b6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1628\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5604\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5605\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5606\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5607\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5608\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys\n",
    "import types\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import misc\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import sklearn\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "df = pd.read_excel('/storage/data.xlsx').drop('Date',axis=1)[:3770].fillna(method='ffill').astype('float64')\n",
    "\n",
    "def test_step(modell):\n",
    "    for it,data in enumerate(iter(modell.test_data),start=0):\n",
    "        model_inp,test_out = data\n",
    "        if it is 0:\n",
    "            model_out_conc = tf.reshape(modell.modell(model_inp,training = False),shape=(1,3))\n",
    "            test_out_conc = tf.reshape(test_out,shape=(1,3))\n",
    "        else:\n",
    "\n",
    "            model_out_conc = tf.concat((model_out_conc,tf.reshape(modell.modell(model_inp,training=False),shape=(1,3))), axis=0)\n",
    "            test_out = tf.reshape(test_out,shape=(1,3))\n",
    "            test_out_conc = tf.concat((test_out_conc,test_out),axis=0)\n",
    "    return test_out_conc,model_out_conc\n",
    "\n",
    "class helpful:\n",
    "    def Preprocessin(self,windowlength = 22, prediction_length = 3, max_wlength=22, outcol = 3):\n",
    "        df = self.df\n",
    "        dftrain=df.copy()[257:2675]\n",
    "        dfvalid=df.copy()[2675:3500]\n",
    "        dftest = df.copy()[3500:]\n",
    "        trainarray=dftrain.iloc[:,outcol].copy()                                   #extract array of particular feature  \n",
    "        validarray=dfvalid.iloc[:,outcol].copy()                                    \n",
    "        testarray=dftest.iloc[:,outcol].copy()              #,tmean,tvar=tanz(arrayzz[:,3].copy(), True)   \n",
    "        transformerr = sklearn.preprocessing.StandardScaler()\n",
    "        self.transformerr = transformerr.fit(np.array(trainarray).reshape(-1,1))\n",
    "        trainarray = self.transformerr.transform(np.array(trainarray).reshape(-1,1))\n",
    "        validarray = self.transformerr.transform(np.array(validarray).reshape(-1,1))\n",
    "        testarray = self.transformerr.transform(np.array(testarray).reshape(-1,1))\n",
    "\n",
    "\n",
    "        pagestrain = trainarray.shape[0] - max_wlength - prediction_length\n",
    "        pagesvalid = validarray.shape[0]  - max_wlength - prediction_length\n",
    "        pagestest = testarray.shape[0] - max_wlength - prediction_length-2\n",
    "\n",
    "        OUTPUT_train = np.asarray([[np.array(trainarray)[i+k+max_wlength] for i in range(prediction_length)] for k in range( pagestrain)])\n",
    "        OUTPUT_val = np.asarray([[np.array(validarray)[i+k+max_wlength] for i in range(prediction_length)] for k in range(pagesvalid)])\n",
    "        OUTPUT_test = np.asarray([[ np.array(testarray)[i+k+max_wlength] for i in range(prediction_length )] for k in range(pagestest)])\n",
    "        for feature in range(np.array(dftrain).shape[1]):\n",
    "            sys.stdout.write(\"\\r{}\" .format(feature/dftrain.shape[1]))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            trainarray=np.asarray(dftrain.iloc[:,feature].copy())                                   #extract array of particular feature  \n",
    "            validarray=np.asarray(dfvalid.iloc[:,feature].copy())                                    \n",
    "            testarray=np.asarray(dftest.iloc[:,feature].copy())  \n",
    "            transformer = sklearn.preprocessing.StandardScaler()\n",
    "            transformer = transformer.fit(np.array(trainarray).reshape(-1,1))\n",
    "            trainarray = transformer.transform(np.array(trainarray).reshape(-1,1))\n",
    "            validarray = transformer.transform(np.array(validarray).reshape(-1,1))\n",
    "            testarray = transformer.transform(np.array(testarray).reshape(-1,1))\n",
    "            del transformer\n",
    "\n",
    "\n",
    "            #trainarray, mean, std = tanz(np.array(trainarray))\n",
    "            #validarray = tanzws(np.array(validarray),mean,std)\n",
    "            #testarray = tanzws(np.array(testarray),mean,std)\n",
    "            #save 2d time series data as  3d list with batch_size=windowlength\n",
    "            trainarray = np.array([[[ np.array(trainarray)[i+k+max_wlength-windowlength] for i in  range(windowlength)] for t in range(1)] for k in range(pagestrain)])\n",
    "            validarray = np.array([[[np.array(validarray)[i+k+max_wlength-windowlength] for i in range(windowlength)] for t in range(1)] for k in range(pagesvalid)]) \n",
    "            testarray = np.array([[[ np.array(testarray)[i+k+max_wlength-windowlength] for i in range(windowlength)]for t in range(1)] for k in range(pagestest)])\n",
    "            if feature == 0:\n",
    "                INPUT_train_lstm = trainarray.copy()\n",
    "                INPUT_valid_lstm = validarray.copy()\n",
    "                INPUT_test_lstm = testarray.copy()\n",
    "            else:\n",
    "                INPUT_train_lstm = np.append(INPUT_train_lstm,trainarray,axis=1)\n",
    "                INPUT_valid_lstm = np.append(INPUT_valid_lstm,validarray,axis=1)\n",
    "                INPUT_test_lstm=np.append(INPUT_test_lstm,testarray,axis=1)\n",
    "\n",
    "        INPUT_train_lstm=INPUT_train_lstm.swapaxes(1,2)\n",
    "        INPUT_valid_lstm=INPUT_valid_lstm.swapaxes(1,2)\n",
    "        INPUT_test_lstm=INPUT_test_lstm.swapaxes(1,2)\n",
    "        INPUT_test_lstm[0,0]\n",
    "        return  np.squeeze(INPUT_train_lstm),np.squeeze(INPUT_test_lstm),np.squeeze(INPUT_valid_lstm),np.squeeze(OUTPUT_val), np.squeeze(np.array(OUTPUT_train)), np.squeeze(np.array(OUTPUT_test))\n",
    "\n",
    "\n",
    "    def parallel_window(self,batchsize = 48, windowlength=[22,14], prediction_length=3,max_wlength=22,outcol=3):\n",
    "        inptrain22, inptest22 , inpvalid22, outvalid, outtrain, outtest = self.Preprocessin(windowlength=windowlength[0]) \n",
    "        inptrain22.shape\n",
    "        inptrain14, inptest14 , inpvalid14, outvalid14, outtrain14, outtest14 = self.Preprocessin(windowlength=windowlength[1]) \n",
    "        inptrain = np.concatenate([inptrain22,inptrain14],axis=1).astype('float64')\n",
    "        inptest = np.concatenate([inptest22,inptest14],axis=1).astype('float64')\n",
    "        inpvalid = np.concatenate([inpvalid22,inpvalid14],axis=1).astype('float64')\n",
    "\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((inptrain,outtrain))\n",
    "        self.train_data = train_data.cache().batch(batchsize).repeat(1)\n",
    "        valid_data = tf.data.Dataset.from_tensor_slices((inpvalid,outvalid))\n",
    "        self.valid_data = valid_data.cache().batch(batchsize).repeat(1)    \n",
    "        test_data = tf.data.Dataset.from_tensor_slices((inptest,outtest))\n",
    "        self.test_data = test_data.cache().batch(1).repeat(1)\n",
    "        \n",
    "    def windowbatch(self, windowlength=22, prediction_length=3,max_wlength=22,outcol=3):\n",
    "        inptrain, inptest , inpvalid, outvalid, outtrain, outtest = self.Preprocessin() \n",
    "\n",
    "\n",
    "        train_data = tf.cast(tf.data.Dataset.from_tensor_slices((inptrain,outtrain)),tf.float64)\n",
    "        self.train_data = train_data.cache().batch(64).repeat(1)\n",
    "        valid_data = tf.cast(tf.data.Dataset.from_tensor_slices((inpvalid,outvalid)),tf.float64)\n",
    "        self.valid_data = valid_data.cache().batch(64).repeat(1)    \n",
    "        test_data = tf.cast(tf.data.Dataset.from_tensor_slices((inptest,outtest)),tf.float64)\n",
    "        self.test_data = test_data.cache().batch(1).repeat(1)\n",
    "\n",
    "        print('\\r all data batched and ready \\n')\n",
    "\n",
    "\"\"\"        \n",
    "        #trainarray, self.train_mean, self.train_std = tanz(np.array(trainarray))\n",
    "        #validarray = tanzws(np.array(validarray),self.train_mean,self.train_std)\n",
    "        #testarray = tanzws(np.array(testarray),self.train_mean,self.train_std)\n",
    "        transformer = sklearn.preprocessing.StandardScaler()\n",
    "        self.transformer = transformer.fit(np.array(trainarray).reshape(-1,1))\n",
    "        trainarray = self.transformer.transform(np.array(trainarray).reshape(-1,1))\n",
    "        validarray = self.transformer.transform(np.array(validarray).reshape(-1,1))\n",
    "        testarray = self.transformer.transform(np.array(testarray).reshape(-1,1))\n",
    "\"\"\"\n",
    "\n",
    "class MODELL(helpful):\n",
    "    def __init__(self):\n",
    "        self.batchsize = 16\n",
    "        self.train_mean = None\n",
    "        self.train_std = None\n",
    "        self.train_data = None\n",
    "        self.valid_data = None\n",
    "        self.test_data = None\n",
    "        self.epochz = None\n",
    "        self.df = df\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.000028, decay=0.7,momentum = 0.99)\n",
    "        self.initz = tf.keras.initializers.glorot_uniform(seed=None)\n",
    "        self.loss_mse = tf.keras.losses.MeanSquaredError()\n",
    "        self.transformerr = None\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name='loss_metric')\n",
    "        self.loss_metric_valid = tf.keras.metrics.Mean(name='loss_metric_valid')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def model_parallel(self):\n",
    "        inp = tf.keras.layers.Input(shape=(36,df.shape[1]))\n",
    "        x1,x2 = tf.split(inp,[22,14],axis=1)\n",
    "        x1 = tf.keras.layers.Conv1D(480,kernel_initializer=self.initz,kernel_size=5)(x1)\n",
    "        x1\n",
    "        x1 = tf.keras.layers.LeakyReLU(alpha=0.4)(x1)\n",
    "        x1\n",
    "        x1 = tf.keras.layers.Conv1D(264,kernel_initializer=self.initz,kernel_size=4)(x1)\n",
    "        \n",
    "        x1 = tf.keras.layers.LeakyReLU(alpha=0.25)(x1)\n",
    "        \n",
    "        x1 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=None)(x1)\n",
    "        \n",
    "        x1 = tf.keras.layers.Conv1D(148,kernel_initializer=self.initz,kernel_size=3)(x1)\n",
    "        \n",
    "        x1 = tf.keras.layers.LeakyReLU(alpha=0.15)(x1)\n",
    "        \n",
    "        x1 = tf.keras.layers.LSTM(128,kernel_initializer=self.initz, return_sequences = True)(x1)\n",
    "        \n",
    "        x1 = tf.keras.layers.LeakyReLU(alpha=0.25)(x1)\n",
    "\n",
    "        x2 = tf.keras.layers.Conv1D(256,kernel_initializer=self.initz,kernel_size=2)(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha=0.4)(x2)\n",
    "        x2 = tf.keras.layers.Conv1D(128,kernel_initializer=self.initz,kernel_size=2)(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha=0.2)(x2)\n",
    "        x2 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=None)(x2)\n",
    "        x2 = tf.keras.layers.Conv1D(96,kernel_initializer=self.initz,kernel_size=2)(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha=0.1)(x2)\n",
    "        x2 = tf.keras.layers.LSTM(256,kernel_initializer=self.initz, return_sequences = True)(x2)\n",
    "        x2\n",
    "        x2 = tf.keras.layers.LeakyReLU(alpha=0.15)(x2)\n",
    "        x1 = tf.keras.layers.Flatten()(x1)\n",
    "        x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "        x = tf.concat([x1,x2],axis=1)\n",
    "        x = tf.keras.layers.Dense(128,use_bias = False)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.15)(x)\n",
    "        x = tf.keras.layers.Dropout(0.45)(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(64,use_bias = False)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.15)(x)\n",
    "        x = tf.keras.layers.Dropout(0.35)(x)\n",
    "\n",
    "        out = tf.keras.layers.Dense(3,use_bias = False)(x)\n",
    "        self.modell=tf.keras.Model(inp,out)\n",
    "        self.modell_weightz = self.modell.trainable_variables\n",
    "\n",
    "    def inverse_transform(self):\n",
    "        return self.inversetanz(self.model_test_pred,self.train_mean,self.train_std), self.inversetanz(self.model_test_out,self.train_mean,self.train_std)\n",
    "\n",
    "    def reset_weights_and_model(self):\n",
    "        self.modell.trainable_weights = self.modell_weightz\n",
    "    \n",
    "\n",
    "    #@tf.function\n",
    "    def train_step(self,data):\n",
    "        inp,real_out = data\n",
    "        with tf.GradientTape() as lstm_tape:\n",
    "            model_out = self.modell(inp,training=True)\n",
    "            loss = tf.cast(self.loss_mse(model_out, real_out),tf.float64)\n",
    "\n",
    "        gradients = lstm_tape.gradient(loss, self.modell.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.modell.trainable_variables))\n",
    "        return self.loss_metric(loss)\n",
    "    \n",
    "    #@tf.function\n",
    "    def valid_step(self,data_v):\n",
    "        inp_train_v,real_out_v = data_v\n",
    "        model_out_v = self.modell(inp_train_v,training=False)\n",
    "        loss_value_v = self.loss_mse(model_out_v, real_out_v)\n",
    "        return self.loss_metric_valid(loss_value_v)\n",
    "\n",
    "    \n",
    "    def test_step(self):\n",
    "        for it,data in enumerate(iter(self.test_data),start=0):\n",
    "            model_inp,test_out = data\n",
    "            if it is 0:\n",
    "                model_out_conc = tf.reshape(self.modell(model_inp,training = False),shape=(1,3))\n",
    "                test_out_conc = tf.reshape(test_out,shape=(1,3))\n",
    "            else:\n",
    "                \n",
    "                model_out_conc = tf.concat((model_out_conc,tf.reshape(self.modell(model_inp,training=False),shape=(1,3))), axis=0)\n",
    "                test_out = tf.reshape(test_out,shape=(1,3))\n",
    "                test_out_conc = tf.concat((test_out_conc,test_out),axis=0)\n",
    "        self.model_test_pred = model_out_conc\n",
    "        self.model_test_out = test_out_conc\n",
    "\n",
    "\n",
    "    def model_test_out(self,lose_bias=False):\n",
    "        self.test_step()\n",
    "        testout = self.transformerr.inverse_transform(self.model_test_out)\n",
    "        testpred = self.transformerr.inverse_transform(self.model_test_pred)\n",
    "        real = list()\n",
    "        predz = list()\n",
    "\n",
    "        for i in range(testout.shape[0]):\n",
    "            if i%3 is 0:\n",
    "                real.extend(testout[i])\n",
    "                predz.extend(testpred[i])\n",
    "        predz = np.array(predz)\n",
    "        real = np.array(real)\n",
    "        if lose_bias:\n",
    "            predz = predz - (predz.mean() - real.mean() )\n",
    "        plt.plot(real,label='real')\n",
    "        plt.plot(predz,label='pred')\n",
    "        plt.legend()\n",
    "\n",
    "    \n",
    "    \n",
    "    def trainingz(self):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "        test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "        valid_log_dir = 'logs/gradient_tape/' + current_time + '/valid'\n",
    "        train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "        test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "        valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)\n",
    "\n",
    "        for epoch in range(self.epochz):\n",
    "            start = time.time()\n",
    "\n",
    "            for data in self.train_data:\n",
    "                loss_ = self.train_step(data)              \n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss_metric', self.loss_metric.result(), step=epoch)\n",
    "\n",
    "\n",
    "            for data_v in self.valid_data:\n",
    "                self.valid_step(data_v)\n",
    "            with valid_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss_metric_valid', self.loss_metric_valid.result(), step=epoch)\n",
    "\n",
    "            template = 'Sec : {} \\n Epoch {} ---- Loss: {}  ----  Val_Loss: {}'\n",
    "            tf.print('Epoch', epoch, ': Time', time.time()-start, ': loss', self.loss_metric.result(), '; valid_loss', self.loss_metric_valid.result())\n",
    "            self.loss_metric.reset_states()\n",
    "            self.loss_metric_valid.reset_states()\n",
    "        return epoch, self.loss_metric.result() , self.loss_metric_valid.result()\n",
    "modell = MODELL()\n",
    "modell.model_parallel()\n",
    "modell.epochz = 3\n",
    "modell.df = df\n",
    "modell.parallel_window()\n",
    "modell.trainingz()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
